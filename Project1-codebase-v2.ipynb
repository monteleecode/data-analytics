{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "105e5337-90fc-423c-917f-cbc40834f2c5",
   "metadata": {},
   "source": [
    "## Step 0: Understand the Data\n",
    "Get the info of errors and missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "c0042fd1-fce7-4ca2-90e2-2b545e7cb2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mktmo\\AppData\\Local\\Temp\\ipykernel_19564\\1985055591.py:3: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  s = pd.read_csv(\"Public_School_Characteristics_2022-23.csv\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(101390, 77)"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "s = pd.read_csv(\"Public_School_Characteristics_2022-23.csv\")\n",
    "s_b = s.copy()\n",
    "s.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63beb45e-60c1-48d1-b07b-2cfa262b6efc",
   "metadata": {},
   "source": [
    "### Pattern for each column\n",
    "TOTAL = TOTMENROL + TOTFENROL\n",
    "TOTAL = PK + KG + G01 + G02 + G03 + G04 + G05\n",
    "      + G06 + G07 + G08\n",
    "      + G09 + G10 + G11 + G12\n",
    "      + UG + AE\n",
    "      \n",
    "TOTAL = AM + AS + HI + BL + WH + HP + TR\n",
    "\n",
    "Lunch:\n",
    "TOTFRL = FRELCH + REDLCH\n",
    "\n",
    "STUTERATIO = TOTAL / FTE\n",
    "\n",
    "Elementary Schools:\n",
    "G09 + G10 + G11 + G12 ≈ 0\n",
    "\n",
    "High Schools:\n",
    "PK + KG + G01–G08 ≈ 0\n",
    "\n",
    "AMALM + AMALF = AM\n",
    "\n",
    "ASALM + ASALF = AS\n",
    "\n",
    "BLALM + BLALF = BL\n",
    "\n",
    "HPALM + HPALF = HP\n",
    "\n",
    "HIALM + HIALF = HI\n",
    "\n",
    "TRALM + TRALF = TR\n",
    "\n",
    "WHALM + WHALF = WH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42acecb7-7297-4772-af85-11a604f4e683",
   "metadata": {},
   "source": [
    "### Missing Value info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "7cd82e03-da69-4216-b044-7530ebb00b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62134\n",
      "101390\n",
      "1352278\n"
     ]
    }
   ],
   "source": [
    "special_values = [-1, -2, -9, \"M\", \"N\"]\n",
    "s = s.replace(r'^\\s*$', np.nan, regex=True) # Find any string that is completely empty ('') or only spaces (' '), replace it with np.nan\n",
    "\n",
    "rows_with_special = s.isin(special_values).any(axis=1).sum()\n",
    "print(rows_with_special)\n",
    "rows_with_missing_value = s.isna().any(axis=1).sum()\n",
    "print(rows_with_missing_value)\n",
    "total_missing = s.isna().sum().sum()\n",
    "print(total_missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea2d878-c3cd-4a1a-9927-7f4c3ed80a86",
   "metadata": {},
   "source": [
    "## Step 1: Data Cleaning\n",
    "### 1.1 Handling missing 0's for Students Each Grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "fd7edc85-7755-41d2-a205-469476202e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101244\n",
      "238819\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cols = s.loc[:, 'PK':'AE'].columns  # all columns from PK to AE\n",
    "\n",
    "# 1️⃣ Row-wise sum of PK to AE (ignores NaN by default)\n",
    "row_sum = s[cols].sum(axis=1)\n",
    "\n",
    "# 2️⃣ Find rows where sum equals TOTAL\n",
    "mask = row_sum.eq(s['TOTAL'])\n",
    "\n",
    "# 3️⃣ Fill NaN with 0 only for those rows\n",
    "s.loc[mask, cols] = s.loc[mask, cols].fillna(0)\n",
    "print(s.isna().any(axis=1).sum())\n",
    "print(s.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c6b271-c824-4f16-88cc-4a89c9fd4459",
   "metadata": {},
   "source": [
    "### 2.2 Removing Redundant Rows and Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ec4995-7a04-4674-83c3-d98b40215880",
   "metadata": {},
   "source": [
    "#### Dropping Rows and Features with Too less value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "ff9b0643-1643-48b6-b765-341a13352c1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49    0.181818\n",
       "50    0.571429\n",
       "51    0.571429\n",
       "52    0.571429\n",
       "53    0.571429\n",
       "54    0.571429\n",
       "55    0.571429\n",
       "dtype: float64"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reference for rows missing value threshold\n",
    "s_b.loc[49:55,:].isna().mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "7d4a968e-13e6-41cb-9be7-eb74551caef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99577\n",
      "164969\n",
      "LSTREET2      99215\n",
      "LZIP4         41866\n",
      "FTE            2565\n",
      "STUTERATIO     1814\n",
      "HPALM           941\n",
      "              ...  \n",
      "NMCNTY            0\n",
      "TOTFRL            0\n",
      "FRELCH            0\n",
      "REDLCH            0\n",
      "LONCOD            0\n",
      "Length: 77, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "s = s[s.isna().mean(axis=1) <= 0.55]\n",
    "print(s.isna().any(axis=1).sum())\n",
    "print(s.isna().sum().sum())\n",
    "print(s.isna().sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "630db254-4478-4115-9e87-affacebc9306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5403\n",
      "23888\n",
      "60467\n"
     ]
    }
   ],
   "source": [
    "s = s.loc[:, s.isna().mean() <= 0.40]\n",
    "print(s.isna().any(axis=1).sum())\n",
    "print(s.isna().sum().sum())\n",
    "print(s.isin(special_values).any(axis=1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "5b597ac7-e67a-4490-895c-2189f1d647ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "s.to_csv(\"Deliverable2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a4f1b7-7cf8-4e32-a5a3-d90e2c7ba2c2",
   "metadata": {},
   "source": [
    "### 2.3 Imputate Continuous Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "8cbc4c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing counts (after placeholder -> NaN):\n",
      "DIRECTCERT    49548\n",
      "FRELCH        18960\n",
      "REDLCH        18960\n",
      "TOTFRL        13098\n",
      "STUTERATIO     4383\n",
      "FTE            2565\n",
      "TOTAL             4\n",
      "dtype: int64\n",
      "Filled TOTAL from grade sums: 4\n",
      "mean abs diff = 0.0063\n",
      "Filled STUTERATIO values: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mktmo\\AppData\\Local\\Temp\\ipykernel_19564\\2411366508.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp[\"ABS_DIFF\"] = (temp[\"STUTERATIO\"] - temp[\"RATIO_FROM_TOTAL_FTE\"]).abs()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filled FTE values: 0\n",
      "\n",
      "Missing counts after continuous imputations:\n",
      "TOTAL          4\n",
      "FTE            4\n",
      "STUTERATIO    55\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# NCES placeholder codes (-1, -2, -9) are non observations (missing / not applicable / low quality),\n",
    "# so they shouldn't be used as real numeric values in statistics and be replaced.\n",
    "\n",
    "placeholder_codes = [-1, -2, -9, \"M\", \"N\"]\n",
    "\n",
    "# Replace placeholder codes with NaN (only for columns where codes appear)\n",
    "for c in [\"STUTERATIO\", \"TOTFRL\", \"FRELCH\", \"REDLCH\", \"DIRECTCERT\", \"CHARTER_TEXT\"]:\n",
    "    if c in s.columns:\n",
    "        s[c] = s[c].replace(placeholder_codes, np.nan)\n",
    "\n",
    "# Ensure numeric columns are numeric\n",
    "numeric_fix_cols = [\n",
    "    \"TOTAL\", \"FTE\", \"STUTERATIO\",\n",
    "    \"TOTFRL\", \"FRELCH\", \"REDLCH\", \"DIRECTCERT\"\n",
    "]\n",
    "for c in numeric_fix_cols:\n",
    "    if c in s.columns:\n",
    "        s[c] = pd.to_numeric(s[c], errors=\"coerce\")\n",
    "\n",
    "print(\"Missing counts (after placeholder -> NaN):\")\n",
    "print(s[numeric_fix_cols].isna().sum().sort_values(ascending=False))\n",
    "\n",
    "# TOTAL: If TOTAL is missing, rebuild from grade level columns (PK to AE) when possible.\n",
    "grade_cols = [c for c in s.columns if c in list(s.loc[:, \"PK\":\"AE\"].columns)]\n",
    "if \"TOTAL\" in s.columns and grade_cols:\n",
    "    total_from_grades = s[grade_cols].sum(axis=1, min_count=1)\n",
    "    missing_total = s[\"TOTAL\"].isna()\n",
    "    s.loc[missing_total, \"TOTAL\"] = total_from_grades[missing_total]\n",
    "    print(\"Filled TOTAL from grade sums:\", int(missing_total.sum()))\n",
    "\n",
    "# STUTERATIO: If missing, compute using TOTAL and FTE when both exist.\n",
    "# First, we check that the relationship holds for most records.\n",
    "if \"TOTAL\" in s.columns and \"FTE\" in s.columns and \"STUTERATIO\" in s.columns:\n",
    "    s[\"RATIO_FROM_TOTAL_FTE\"] = s[\"TOTAL\"] / s[\"FTE\"]\n",
    "    s.loc[s[\"FTE\"] == 0, \"RATIO_FROM_TOTAL_FTE\"] = None\n",
    "    temp = s.dropna(subset=[\"STUTERATIO\", \"RATIO_FROM_TOTAL_FTE\"])\n",
    "    temp[\"ABS_DIFF\"] = (temp[\"STUTERATIO\"] - temp[\"RATIO_FROM_TOTAL_FTE\"]).abs()\n",
    "    print(\"mean abs diff =\", round(float(temp[\"ABS_DIFF\"].mean()), 4))\n",
    "    before = s[\"STUTERATIO\"].isna().sum()\n",
    "    s.loc[s[\"STUTERATIO\"].isna(), \"STUTERATIO\"] = s[\"RATIO_FROM_TOTAL_FTE\"]\n",
    "    after = s[\"STUTERATIO\"].isna().sum()\n",
    "    print(\"Filled STUTERATIO values:\", int(before - after))\n",
    "    s = s.drop(columns=[\"RATIO_FROM_TOTAL_FTE\"], errors=\"ignore\")\n",
    "# To verify consistency internally, we compared the STUTERATIO to the computed value TOTAL/FTE. \n",
    "# We calculated the mean absolute difference to measure the avg deviation between the two values. \n",
    "# A small mean absolute difference shows that the ratios are consistent with actual enrollment and teacher counts.\n",
    "\n",
    "# FTE: If missing, compute using TOTAL / STUTERATIO.\n",
    "if \"TOTAL\" in s.columns and \"FTE\" in s.columns and \"STUTERATIO\" in s.columns:\n",
    "    s[\"FTE_FROM_RATIO\"] = s[\"TOTAL\"] / s[\"STUTERATIO\"]\n",
    "    s.loc[s[\"STUTERATIO\"] == 0, \"FTE_FROM_RATIO\"] = None\n",
    "    before = s[\"FTE\"].isna().sum()\n",
    "    s.loc[s[\"FTE\"].isna(), \"FTE\"] = s[\"FTE_FROM_RATIO\"]\n",
    "    after = s[\"FTE\"].isna().sum()\n",
    "    print(\"Filled FTE values:\", int(before - after))\n",
    "    s = s.drop(columns=[\"FTE_FROM_RATIO\"], errors=\"ignore\")\n",
    "\n",
    "# Remaining missing in continuous columns: use group wise median since less sensitive to outliers\n",
    "# Group by SCHOOL_LEVEL and STABR if available because school staffing and enrollment patterns are different depending on level and state.\n",
    "for c in [\"STUTERATIO\", \"FTE\"]:\n",
    "    if c in s.columns:\n",
    "        # If both grouping columns exist, fill by SCHOOL_LEVEL and STABR\n",
    "        if \"SCHOOL_LEVEL\" in s.columns and \"STABR\" in s.columns:\n",
    "            for level in s[\"SCHOOL_LEVEL\"].dropna().unique():\n",
    "                for state in s[\"STABR\"].dropna().unique():\n",
    "                    # rows in this level and this state\n",
    "                    condition = (s[\"SCHOOL_LEVEL\"] == level) & (s[\"STABR\"] == state)\n",
    "                    group_values = s.loc[condition, c]\n",
    "                    # calculate median if the group has at least one proper value\n",
    "                    if group_values.notna().sum() > 0:\n",
    "                        group_median = group_values.median()\n",
    "                        # fill only missing values inside this group\n",
    "                        s.loc[condition & s[c].isna(), c] = group_median\n",
    "        # Otherwise, fill it with overall median\n",
    "        else:\n",
    "            overall_median = s[c].median()\n",
    "            s.loc[s[c].isna(), c] = overall_median\n",
    "# Remaining missing values in continuous variables (STUTERATIO and FTE) were imputed using group-wise median imputation. \n",
    "# Schools were grouped by SCHOOL_LEVEL and STABR because staffing and enrollment differ across different school types and states. \n",
    "# Median was chosen instead of mean to minimize sensitivity to extreme outliers in the enrollment and staffing counters. \n",
    "# This helps to uphold differences in the structure of the dataset but preventing too much data loss from deleting rows. \n",
    "# If grouping variables were unavailable, overall median imputation was used.\n",
    "\n",
    "print(\"\\nMissing counts after continuous imputations:\")\n",
    "print(s[[\"TOTAL\", \"FTE\", \"STUTERATIO\"]].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e163744b-1eaa-445c-8e83-844c0ba72a4f",
   "metadata": {},
   "source": [
    "### 2.4 Imputate Discrete Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "e4e01aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done discrete repairs. Remaining missing values (top 15):\n",
      "DIRECTCERT      49548\n",
      "CHARTER_TEXT     3681\n",
      "HPALM             941\n",
      "HPALF             940\n",
      "AMALM             914\n",
      "AMALF             912\n",
      "HP                894\n",
      "AM                866\n",
      "BLALF             830\n",
      "BLALM             827\n",
      "ASALM             825\n",
      "ASALF             823\n",
      "TRALM             820\n",
      "BL                820\n",
      "TRALF             818\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Fix postal codes: if LZIP is numeric, leading zeros are lost in python. This converts them back to 5 character string.\n",
    "if \"LZIP\" in s.columns:\n",
    "    s[\"LZIP\"] = s[\"LZIP\"].astype(\"Int64\").astype(str).str.zfill(5)\n",
    "\n",
    "# Address field: if missing, fill with 'Unknown'\n",
    "for c in [\"LSTREET1\", \"PHONE\"]:\n",
    "    if c in s.columns:\n",
    "        s[c] = s[c].fillna(\"Unknown\")\n",
    "# Missing values in categorical variables not included in the analysis such as street address and phone number were replaced \n",
    "# with the placeholder value \"Unknown\". These variables are descriptive identifiers and are not used for analysis. \n",
    "# Replacing missing values prevents needing to delete rows unnecessarily due to empty values and ensures consistency in the dataset.\n",
    "\n",
    "# Free/Reduced lunch fields: these are count fields and can be 0 for schools with no participants.\n",
    "# We used enrollment percentage as a consideration to avoid producing impossible counts.\n",
    "if \"TOTFRL\" in s.columns and \"TOTAL\" in s.columns:\n",
    "    # Create FRL_PCT (only when TOTAL > 0)\n",
    "    s[\"FRL_PCT\"] = s[\"TOTFRL\"] / s[\"TOTAL\"]\n",
    "    s.loc[s[\"TOTAL\"] == 0, \"FRL_PCT\"] = None\n",
    "    # Fill missing FRL_PCT with overall median\n",
    "    frl_median = s[\"FRL_PCT\"].median()\n",
    "    s.loc[s[\"FRL_PCT\"].isna(), \"FRL_PCT\"] = frl_median\n",
    "    # missing TOTFRL using FRL_PCT * TOTAL\n",
    "    s.loc[s[\"TOTFRL\"].isna(), \"TOTFRL\"] = (s[\"FRL_PCT\"] * s[\"TOTAL\"]).round()\n",
    "    # keep TOTFRL within valid range [0, TOTAL]\n",
    "    s.loc[s[\"TOTFRL\"] < 0, \"TOTFRL\"] = 0\n",
    "    s.loc[s[\"TOTFRL\"] > s[\"TOTAL\"], \"TOTFRL\"] = s[\"TOTAL\"]\n",
    "# if FRELCH and REDLCH exist, fill missing by splitting TOTFRL 50/50\n",
    "if \"FRELCH\" in s.columns and \"REDLCH\" in s.columns and \"TOTFRL\" in s.columns:\n",
    "    # If missing, assign half of TOTFRL to free lunch\n",
    "    s.loc[s[\"FRELCH\"].isna(), \"FRELCH\"] = (0.5 * s[\"TOTFRL\"]).round()\n",
    "    # Reduced lunch is whatever is left\n",
    "    s.loc[s[\"REDLCH\"].isna(), \"REDLCH\"] = (s[\"TOTFRL\"] - s[\"FRELCH\"]).round()\n",
    "    # Prevent negatives\n",
    "    s.loc[s[\"REDLCH\"] < 0, \"REDLCH\"] = 0\n",
    "# The Free/Reduced Lunch variables are count fields, so they must be not negative and not greater than total enrollment.\n",
    "# To handle missing values first, we converted the total FRL count into a percentage of enrollment so that any imputed values \n",
    "# would scale properly with school size. Missing percentages were filled using the median to avoid being influenced by extreme values. \n",
    "# Then we rebuilt missing FRL counts using the percentage and total enrollment, rounding to keep whole numbers. \n",
    "# More checks made sure values stayed between 0 and total enrollment. When the free and reduced breakdown was missing,\n",
    "# we used a 50/50 split to maintain consistency. These steps ensured logical modification while keeping as much data as possible.\n",
    "\n",
    "# Ensure counts are non-negative\n",
    "count_cols = [\"TOTAL\", \"TOTFRL\", \"FRELCH\", \"REDLCH\", \"DIRECTCERT\", \"TOTMENROL\", \"TOTFENROL\", \"MEMBER\"]\n",
    "\n",
    "for c in count_cols:\n",
    "    if c in s.columns:\n",
    "        s[c] = pd.to_numeric(s[c], errors=\"coerce\")\n",
    "        s.loc[s[c] < 0, c] = 0\n",
    "\n",
    "print(\"Done discrete repairs. Remaining missing values (top 15):\")\n",
    "print(s.isna().sum().sort_values(ascending=False).head(15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "ee10308f-a761-4b05-b2f0-0096e5d57bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constant columns: ['CHARTER_TEXT', 'STATUS', 'FRL_PCT']\n"
     ]
    }
   ],
   "source": [
    "# Handling DIRECTCERT\n",
    "## make sure to convert all to numeric\n",
    "s['DIRECTCERT'] = pd.to_numeric(s['DIRECTCERT'], errors='coerce')\n",
    "s['FRELCH'] = pd.to_numeric(s['FRELCH'], errors='coerce')\n",
    "\n",
    "## FRELCH = Free lunch eligible, is high related to DIRECTCERT, so we use it to give an estimation of DIRECTCERT\n",
    "## get the median ratio of \"DIRECTVERT\"/\"FRELCH\"\n",
    "D_ratio = s['DIRECTCERT'] / s['FRELCH']\n",
    "D_median_ratio = D_ratio[\n",
    "    (s['DIRECTCERT'].notna()) & \n",
    "    (s['FRELCH'].notna()) &\n",
    "    (s['FRELCH'] != 0)\n",
    "].median()\n",
    "\n",
    "## give missing value of missing DIRECTCERT only if FRELCH exists\n",
    "D_mask = s['DIRECTCERT'].isna() & s['FRELCH'].notna()\n",
    "s.loc[D_mask, 'DIRECTCERT'] = s.loc[D_mask, 'FRELCH'] * D_median_ratio\n",
    "\n",
    "# Handling CHARTER_TEXT\n",
    "## the value is mostly NO, so check the constant extent\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "constant_threshold = 0.2\n",
    "s['CHARTER_TEXT'] = s['CHARTER_TEXT'].map({\n",
    "    'Yes': 1,\n",
    "    'No': 0\n",
    "})\n",
    "num_df = s.select_dtypes(include='number')\n",
    "selector = VarianceThreshold(constant_threshold)\n",
    "selector.fit(num_df)\n",
    "kept_cols = num_df.columns[selector.get_support()]\n",
    "constant_cols = num_df.columns[~selector.get_support()]\n",
    "print(\"Constant columns:\", list(constant_cols))\n",
    "# it's constant so will drop the feature at the step of handling constant feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "fab8960d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with all race counts missing: 813\n",
      "Rows where race sum not equal TOTAL (flagged): 5900\n",
      "Rows where male+female != TOTAL: 5978\n",
      "\n",
      "Remaining missing values (top 15):\n",
      "CHARTER_TEXT    3680\n",
      "HPALF             61\n",
      "HPALM             60\n",
      "STUTERATIO        49\n",
      "AMALM             41\n",
      "AMALF             36\n",
      "BLALF              5\n",
      "ASALM              5\n",
      "AE                 5\n",
      "UG                 5\n",
      "G13                5\n",
      "G09                4\n",
      "G11                4\n",
      "BLALM              4\n",
      "G12                4\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Cleaning up the races value\n",
    "# Goal: ensure totals are internally consistent and remove/repair impossible records.\n",
    "\n",
    "race_cols = [\"AM\",\"AS\",\"BL\",\"HI\",\"HP\",\"TR\",\"WH\"]\n",
    "race_cols = [c for c in race_cols if c in s.columns]\n",
    "\n",
    "# Convert to numeric\n",
    "for c in race_cols:\n",
    "    s[c] = pd.to_numeric(s[c], errors=\"coerce\")\n",
    "\n",
    "# Race counts: if ALL race columns are missing, ethnicity composition cannot be computed.\n",
    "# Since our problem statement uses ethnicity composition, we drop these rows.\n",
    "all_race_missing = s[race_cols].isna().all(axis=1) if len(race_cols) > 0 else pd.Series(False, index=s.index)\n",
    "print(\"Rows with all race counts missing:\", int(all_race_missing.sum()))\n",
    "s = s.loc[~all_race_missing].copy()\n",
    "\n",
    "# If some race categories are missing but the known categories already sum to TOTAL, then missing categories must be 0.\n",
    "if \"TOTAL\" in s.columns:\n",
    "\n",
    "    race_sum_known = s[race_cols].sum(axis=1, min_count=1)\n",
    "\n",
    "    # Loop through each row\n",
    "    race_na_any = s[race_cols].isna().any(axis=1)\n",
    "    for i in s.index:\n",
    "        if (race_na_any.at[i] and pd.notna(s.at[i, \"TOTAL\"]) and race_sum_known.at[i] == s.at[i, \"TOTAL\"]):\n",
    "            s.loc[i, race_cols] = s.loc[i, race_cols].fillna(0)\n",
    "\n",
    "    # If exactly ONE race category is missing and TOTAL is known, fill the missing one as the remainder.\n",
    "    missing_counts = s[race_cols].isna().sum(axis=1)\n",
    "    one_missing = (missing_counts == 1) & s[\"TOTAL\"].notna()\n",
    "    remainder = s[\"TOTAL\"] - s[race_cols].sum(axis=1, min_count=1)\n",
    "\n",
    "    # Identify which column is missing per row and fill it\n",
    "    for c in race_cols:\n",
    "        mask = one_missing & s[c].isna() & (remainder >= 0)\n",
    "        s.loc[mask, c] = remainder[mask]\n",
    "\n",
    "    # Any remaining negative remainder means race totals exceed TOTAL (inconsistent).\n",
    "    ##remainder_after = s[\"TOTAL\"] - s[race_cols].sum(axis=1, min_count=1)\n",
    "    ##inconsistent_race = remainder_after != 0\n",
    "    \n",
    "    ##s = s.loc[~inconsistent_race].copy()\n",
    "    remainder_after = s[\"TOTAL\"] - s[race_cols].sum(axis=1, min_count=1)\n",
    "    s['TOTAL_RACE_CONS'] = remainder_after == 0  # True if sum equals TOTAL, False otherwise\n",
    "    inconsistent_race = remainder_after != 0\n",
    "    \n",
    "    inconsistent_mask = ~s['TOTAL_RACE_CONS']  # rows where sum != TOTAL\n",
    "    cols_to_fill = s.loc[:, 'AMALM':'WH'].columns  # columns from AMALM to WH\n",
    "    s.loc[inconsistent_mask, cols_to_fill] = s.loc[inconsistent_mask, cols_to_fill].fillna(0)\n",
    "    \n",
    "    print(\"Rows where race sum not equal TOTAL (flagged):\", int(inconsistent_race.sum()))\n",
    "    \n",
    "# Gender totals vs TOTAL: if TOTMENROL + TOTFENROL != TOTAL, we keep as is but flag for awareness.\n",
    "if \"TOTMENROL\" in s.columns and \"TOTFENROL\" in s.columns and \"TOTAL\" in s.columns:\n",
    "    condition = (s[\"TOTMENROL\"].notna() & s[\"TOTFENROL\"].notna() & s[\"TOTAL\"].notna())\n",
    "    # Among those rows, check if male + female does NOT equal TOTAL\n",
    "    inconsistent = s.loc[condition, \"TOTMENROL\"] + s.loc[condition, \"TOTFENROL\"] != s.loc[condition, \"TOTAL\"]\n",
    "    print(\"Rows where male+female != TOTAL:\", int(inconsistent.sum()))\n",
    "    \n",
    "# Final missing snapshot\n",
    "print(\"\\nRemaining missing values (top 15):\")\n",
    "print(s.isna().sum().sort_values(ascending=False).head(15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "786e4d93-1d5e-4e60-aaf4-e21e3b6c0ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows where grade sum not equal TOTAL (flagged): 5\n"
     ]
    }
   ],
   "source": [
    "grade_cols = s.loc[:, 'PK':'AE'].columns  # columns from PK to AE\n",
    "remainder_after_grades = s[grade_cols].sum(axis=1, min_count=1) - s['TOTAL']  # or whatever your total column is for grades\n",
    "s['TOTAL_GRADE_CONS'] = remainder_after_grades == 0  # True if sum equals TOTAL, False otherwise\n",
    "inconsistent_grade = remainder_after_grades != 0\n",
    "\n",
    "# --- Step 2: Fill NaN with 0 for inconsistent rows only ---\n",
    "inconsistent_grade_mask = ~s['TOTAL_GRADE_CONS']  # rows where sum != TOTAL\n",
    "s.loc[inconsistent_grade_mask, grade_cols] = s.loc[inconsistent_grade_mask, grade_cols].fillna(0)\n",
    "\n",
    "# --- Step 3: Print summary ---\n",
    "print(\"Rows where grade sum not equal TOTAL (flagged):\", int(inconsistent_grade.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "08e135f8-0abb-4165-9620-601eb2d3ab14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHARTER_TEXT    3680\n",
      "STUTERATIO        49\n",
      "FTE                2\n",
      "BLALF              0\n",
      "AS                 0\n",
      "ASALF              0\n",
      "ASALM              0\n",
      "AM                 0\n",
      "AMALF              0\n",
      "AMALM              0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Handling the rest of race values\n",
    "## for race_cols = [\"AM\",\"AS\",\"BL\",\"HI\",\"HP\",\"TR\",\"WH\"], \n",
    "## get the median of the genders of each race and replace the NAN value if the race_cols are not NAN\n",
    "for race in race_cols:\n",
    "    total_col = race\n",
    "    male_col = f\"{race}ALM\"\n",
    "    female_col = f\"{race}ALF\"\n",
    "    \n",
    "    # --- MALE IMPUTATION ---\n",
    "    male_ratio = s[male_col] / s[total_col]\n",
    "    \n",
    "    male_median_ratio = male_ratio[\n",
    "        (s[male_col].notna()) &\n",
    "        (s[total_col].notna()) &\n",
    "        (s[total_col] != 0)\n",
    "    ].median()\n",
    "    \n",
    "    male_mask = s[male_col].isna() & s[total_col].notna()\n",
    "    s.loc[male_mask, male_col] = (s.loc[male_mask, total_col] * male_median_ratio).round().astype(int)\n",
    "    \n",
    "    # --- FEMALE IMPUTATION ---\n",
    "    female_ratio = s[female_col] / s[total_col]\n",
    "    \n",
    "    female_median_ratio = female_ratio[\n",
    "        (s[female_col].notna()) &\n",
    "        (s[total_col].notna()) &\n",
    "        (s[total_col] != 0)\n",
    "    ].median()\n",
    "    \n",
    "    female_mask = s[female_col].isna() & s[total_col].notna()\n",
    "    s.loc[female_mask, female_col] = (s.loc[female_mask, total_col] * female_median_ratio).round().astype(int)\n",
    "    \n",
    "    # --- ADJUST TO ENSURE MALE + FEMALE = TOTAL ---\n",
    "    # For rows where total is known and one or both genders were imputed\n",
    "    adjust_mask = s[total_col].notna() & s[male_col].notna() & s[female_col].notna()\n",
    "    difference = s.loc[adjust_mask, total_col] - (s.loc[adjust_mask, male_col] + s.loc[adjust_mask, female_col])\n",
    "    \n",
    "    # Add/subtract difference to female to balance exactly\n",
    "    s.loc[adjust_mask, female_col] += difference.astype(int)\n",
    "    \n",
    "print(s.isna().sum().sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "93b02e16-07fe-45a3-9853-bcd29bb0d45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHARTER_TEXT    3680\n",
      "X                  0\n",
      "BLALF              0\n",
      "AS                 0\n",
      "ASALF              0\n",
      "ASALM              0\n",
      "AM                 0\n",
      "AMALF              0\n",
      "AMALM              0\n",
      "STUTERATIO         0\n",
      "FTE                0\n",
      "MEMBER             0\n",
      "TOTAL              0\n",
      "TOTFENROL          0\n",
      "TOTMENROL          0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PK</th>\n",
       "      <th>KG</th>\n",
       "      <th>G01</th>\n",
       "      <th>G02</th>\n",
       "      <th>G03</th>\n",
       "      <th>G04</th>\n",
       "      <th>G05</th>\n",
       "      <th>G06</th>\n",
       "      <th>G07</th>\n",
       "      <th>G08</th>\n",
       "      <th>G09</th>\n",
       "      <th>G10</th>\n",
       "      <th>G11</th>\n",
       "      <th>G12</th>\n",
       "      <th>G13</th>\n",
       "      <th>UG</th>\n",
       "      <th>AE</th>\n",
       "      <th>TOTMENROL</th>\n",
       "      <th>TOTFENROL</th>\n",
       "      <th>TOTAL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [PK, KG, G01, G02, G03, G04, G05, G06, G07, G08, G09, G10, G11, G12, G13, UG, AE, TOTMENROL, TOTFENROL, TOTAL]\n",
       "Index: []"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace the rest NAN STUTERATIO according to TOTAL and FTE\n",
    "## Fill in the rest of the missing FTE\n",
    "\n",
    "## fill in the STUTERATIO\n",
    "mask_fte_known = s['STUTERATIO'].isna() & s['FTE'].notna()\n",
    "s.loc[mask_fte_known, 'STUTERATIO'] = s.loc[mask_fte_known].apply(\n",
    "    lambda row: row['TOTAL'] / row['FTE'] if row['FTE'] != 0 else 0,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Optional: round STUTERATIO\n",
    "s['STUTERATIO'] = s['STUTERATIO'].round(2)\n",
    "\n",
    "# --- Step 2: Fill missing STUTERATIO using group median ---\n",
    "mask_fte_missing = s['STUTERATIO'].isna()\n",
    "group_median_stu = s.groupby(['STABR', 'SCHOOL_LEVEL'])['STUTERATIO'].transform('median')\n",
    "s.loc[mask_fte_missing, 'STUTERATIO'] = s.loc[mask_fte_missing].apply(\n",
    "    lambda row: group_median_stu[row.name] if not pd.isna(group_median_stu[row.name]) else 0,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Optional: round STUTERATIO\n",
    "s['STUTERATIO'] = s['STUTERATIO'].round(2)\n",
    "\n",
    "# --- Step 3: Compute missing FTE from STUTERATIO ---\n",
    "mask_fte_nan = s['FTE'].isna() & s['STUTERATIO'].notna()\n",
    "s.loc[mask_fte_nan, 'FTE'] = s.loc[mask_fte_nan].apply(\n",
    "    lambda row: row['TOTAL'] / row['STUTERATIO'] if row['STUTERATIO'] != 0 else 0,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Optional: round FTE\n",
    "s['FTE'] = s['FTE'].round(2)\n",
    "print(s.isna().sum().sort_values(ascending=False).head(15))\n",
    "s.loc[s[\"G13\"].isna(), \"PK\":\"TOTAL\"]\n",
    "# s.loc[s[\"HP\"].isna(), \"TOTAL\":\"WH\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e5b3e7-91f5-4283-b22e-45822ab3b474",
   "metadata": {},
   "source": [
    "### 2.5 Handling Edge Cases¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d241e9-d42c-4c71-bbb8-a420f33f86e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11cea250-0a98-475e-a627-33d2d6ddf55b",
   "metadata": {},
   "source": [
    "### 2.6 Feature Engineering\n",
    "The features added through the imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b194dade-45fb-47b9-8c7f-84b8e944d12a",
   "metadata": {},
   "source": [
    "# Ethnicity composition as proportions (robust to different school sizes)\n",
    "race_cols = [\"AM\",\"AS\",\"BL\",\"HI\",\"HP\",\"TR\",\"WH\"]\n",
    "race_cols = [c for c in race_cols if c in s.columns]\n",
    "\n",
    "for c in race_cols:\n",
    "    s[c] = pd.to_numeric(s[c], errors=\"coerce\")\n",
    "\n",
    "if \"TOTAL\" in s.columns:\n",
    "    for c in race_cols:\n",
    "        s[f\"{c}_PCT\"] = np.where(s[\"TOTAL\"] > 0, s[c] / s[\"TOTAL\"], np.nan)\n",
    "\n",
    "    # minority share (everything except White)\n",
    "    if \"WH\" in s.columns:\n",
    "        s[\"MINORITY_PCT\"] = 1 - s[\"WH_PCT\"]\n",
    "\n",
    "# School level (ordinal encoding) to support correlation\n",
    "level_map = {\n",
    "    \"Primary\": 1,\n",
    "    \"Middle\": 2,\n",
    "    \"High\": 3,\n",
    "    \"Other\": 0\n",
    "}\n",
    "if \"SCHOOL_LEVEL\" in s.columns:\n",
    "    s[\"SCHOOL_LEVEL_CODE\"] = s[\"SCHOOL_LEVEL\"].map(level_map).fillna(0).astype(int)\n",
    "\n",
    "print(\"Example engineered columns:\")\n",
    "engineered_cols = [c for c in s.columns if c.endswith(\"_PCT\")] + [\"MINORITY_PCT\",\"FRL_PCT\",\"IS_CHARTER\",\"IS_VIRTUAL\",\"SCHOOL_LEVEL_CODE\"]\n",
    "print([c for c in engineered_cols if c in s.columns][:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8044204f-9b9e-4fc9-b440-f5f241045095",
   "metadata": {},
   "source": [
    "### 2.7 Dropping Constant Features and Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "a2f03383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constant columns: ['CHARTER_TEXT', 'STATUS', 'FRL_PCT']\n",
      "Exact duplicate rows removed: 0\n",
      "Duplicate NCESSCH rows removed: 0\n",
      "X             0\n",
      "BLALM         0\n",
      "ASALF         0\n",
      "ASALM         0\n",
      "AM            0\n",
      "AMALF         0\n",
      "AMALM         0\n",
      "STUTERATIO    0\n",
      "FTE           0\n",
      "MEMBER        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Drop fully-constant columns providing no information for model\n",
    "constant_threshold = 0.3\n",
    "num_df = s.select_dtypes(include='number')\n",
    "selector = VarianceThreshold(constant_threshold)\n",
    "selector.fit(num_df)\n",
    "kept_cols = num_df.columns[selector.get_support()]\n",
    "constant_cols = num_df.columns[~selector.get_support()]\n",
    "print(\"Constant columns:\", list(constant_cols))\n",
    "s = s.drop(columns=constant_cols)\n",
    "\n",
    "# >= 99.5% of rows share the same value\n",
    "#quasi_constant_cols = []\n",
    "#for col in s.columns:\n",
    "#    top_freq = s[col].value_counts(dropna=False, normalize=True).iloc[0]\n",
    "    # if top_freq >= 0.995:\n",
    "    #     quasi_constant_cols.append(col)\n",
    "\n",
    "# print(\">=99.5% same:\", quasi_constant_cols)\n",
    "\n",
    "# Remove duplicate records (exact duplicates or duplicate school IDs)\n",
    "before = s.shape[0]\n",
    "s = s.drop_duplicates()\n",
    "after = s.shape[0]\n",
    "print(f\"Exact duplicate rows removed: {before-after}\")\n",
    "\n",
    "if \"NCESSCH\" in s.columns:\n",
    "    before = s.shape[0]\n",
    "    s = s.drop_duplicates(subset=[\"NCESSCH\"])\n",
    "    after = s.shape[0]\n",
    "    print(f\"Duplicate NCESSCH rows removed: {before-after}\")\n",
    "print(s.isna().sum().sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b2ed7e-b1c8-4e43-a379-91ce804838c6",
   "metadata": {},
   "source": [
    "### 2.8 Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "a88791b3-cb82-4506-93c6-9369341354fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AM',\n",
       " 'AMALF',\n",
       " 'AS',\n",
       " 'ASALF',\n",
       " 'BL',\n",
       " 'BLALF',\n",
       " 'DIRECTCERT',\n",
       " 'FRELCH',\n",
       " 'FTE',\n",
       " 'G01',\n",
       " 'G02',\n",
       " 'G03',\n",
       " 'G04',\n",
       " 'G05',\n",
       " 'G07',\n",
       " 'G08',\n",
       " 'G10',\n",
       " 'G11',\n",
       " 'G12',\n",
       " 'HI',\n",
       " 'HIALF',\n",
       " 'HIALM',\n",
       " 'HP',\n",
       " 'HPALF',\n",
       " 'LATCOD',\n",
       " 'LEAID',\n",
       " 'LONCOD',\n",
       " 'MEMBER',\n",
       " 'NCESSCH',\n",
       " 'TOTAL',\n",
       " 'TOTFENROL',\n",
       " 'TOTMENROL',\n",
       " 'TR',\n",
       " 'TRALF',\n",
       " 'WH',\n",
       " 'WHALF'}"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "#Using Pearson Correlation\n",
    "num_df = s.select_dtypes(include='number')\n",
    "\n",
    "# Compute correlation\n",
    "# cor = num_df.corr()\n",
    "\n",
    "# # Plot heatmap\n",
    "# plt.figure(figsize=(8,6))\n",
    "# sns.heatmap(cor, annot=True, cmap=plt.cm.CMRmap_r)\n",
    "# plt.show()\n",
    "\n",
    "def correlation(dataset, threshold):\n",
    "    col_corr = set()  # Set of all the names of correlated columns\n",
    "    corr_matrix = dataset.corr()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n",
    "                colname = corr_matrix.columns[i]  # getting the name of column\n",
    "                col_corr.add(colname)\n",
    "    return col_corr\n",
    "corr_features = correlation(num_df, 0.7)\n",
    "corr_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "e809c794-5d45-4398-9931-16ef6796bfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "s.drop(corr_features,axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8986a41d-dcc4-47b7-a92b-cc912f84b787",
   "metadata": {},
   "source": [
    "### 2.9 Data Shape After Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "744aa415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after cleaning: (98910, 39)\n",
      "Total missing values remaining: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape after cleaning:\", s.shape)\n",
    "print(\"Total missing values remaining:\", int(s.isna().sum().sum()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb0d7b1-d112-4e49-ae54-317cf52848c1",
   "metadata": {},
   "source": [
    "## Step 3: Normalization and Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "f294a441",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Transformation (log1p) for heavy-tailed count features to reduce skew\n",
    "\n",
    "# Standardization (z-score) for continuous features used in modeling\n",
    "\n",
    "# Select a few representative count features\n",
    "count_features = [c for c in [\"TOTAL\",\"FTE\",\"TOTFRL\"] if c in s.columns]\n",
    "for c in count_features:\n",
    "    s[f\"LOG1P_{c}\"] = np.log1p(s[c])\n",
    "\n",
    "# Standardize continuous features\n",
    "cont_features = [c for c in [\"STUTERATIO\",\"FRL_PCT\",\"MINORITY_PCT\",\"TOTAL\",\"FTE\"] if c in s.columns]\n",
    "for c in cont_features:\n",
    "    mean = s[c].mean()\n",
    "    std = s[c].std()\n",
    "    s[f\"Z_{c}\"] = (s[c] - mean) / std\n",
    "\n",
    "# Correlation analysis: which engineered variables correlate with STUTERATIO?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c2c7ae",
   "metadata": {},
   "source": [
    "### 3.1 Export Cleaned Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "2d11da9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Deliverable2_cleaned_final.csv\n"
     ]
    }
   ],
   "source": [
    "s.to_csv(\"Deliverable2_cleaned.csv\", index=False)\n",
    "print(\"Saved Deliverable2_cleaned_final.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff120dcc-2acd-49ab-967e-d60fa240ab34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
