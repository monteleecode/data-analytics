{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "105e5337-90fc-423c-917f-cbc40834f2c5",
   "metadata": {},
   "source": [
    "## Step 0: Understand the Data\n",
    "Get the info of errors and missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0042fd1-fce7-4ca2-90e2-2b545e7cb2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "s = pd.read_csv(\"Public_School_Characteristics_2022-23.csv\")\n",
    "s_b = s.copy()\n",
    "s.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63beb45e-60c1-48d1-b07b-2cfa262b6efc",
   "metadata": {},
   "source": [
    "### Pattern for each column\n",
    "TOTAL = TOTMENROL + TOTFENROL\n",
    "TOTAL = PK + KG + G01 + G02 + G03 + G04 + G05\n",
    "      + G06 + G07 + G08\n",
    "      + G09 + G10 + G11 + G12\n",
    "      + UG + AE\n",
    "      \n",
    "TOTAL = AM + AS + HI + BL + WH + HP + TR\n",
    "\n",
    "Lunch:\n",
    "TOTFRL = FRELCH + REDLCH\n",
    "\n",
    "STUTERATIO = TOTAL / FTE\n",
    "\n",
    "Elementary Schools:\n",
    "G09 + G10 + G11 + G12 ≈ 0\n",
    "\n",
    "High Schools:\n",
    "PK + KG + G01–G08 ≈ 0\n",
    "\n",
    "AMALM + AMALF = AM\n",
    "\n",
    "ASALM + ASALF = AS\n",
    "\n",
    "BLALM + BLALF = BL\n",
    "\n",
    "HPALM + HPALF = HP\n",
    "\n",
    "HIALM + HIALF = HI\n",
    "\n",
    "TRALM + TRALF = TR\n",
    "\n",
    "WHALM + WHALF = WH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42acecb7-7297-4772-af85-11a604f4e683",
   "metadata": {},
   "source": [
    "### Missing Value info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd82e03-da69-4216-b044-7530ebb00b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_values = [-1, -2, -9, \"M\", \"N\"]\n",
    "s = s.replace(r'^\\s*$', np.nan, regex=True) # Find any string that is completely empty ('') or only spaces (' '), replace it with np.nan\n",
    "\n",
    "rows_with_special = s.isin(special_values).any(axis=1).sum()\n",
    "print(rows_with_special)\n",
    "rows_with_missing_value = s.isna().any(axis=1).sum()\n",
    "print(rows_with_missing_value)\n",
    "total_missing = s.isna().sum().sum()\n",
    "print(total_missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea2d878-c3cd-4a1a-9927-7f4c3ed80a86",
   "metadata": {},
   "source": [
    "## Step 1: Data Cleaning\n",
    "### 1.1 Handling missing 0's for Students Each Grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7edc85-7755-41d2-a205-469476202e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cols = s.loc[:, 'PK':'AE'].columns  # all columns from PK to AE\n",
    "\n",
    "# 1️⃣ Row-wise sum of PK to AE (ignores NaN by default)\n",
    "row_sum = s[cols].sum(axis=1)\n",
    "\n",
    "# 2️⃣ Find rows where sum equals TOTAL\n",
    "mask = row_sum.eq(s['TOTAL'])\n",
    "\n",
    "# 3️⃣ Fill NaN with 0 only for those rows\n",
    "s.loc[mask, cols] = s.loc[mask, cols].fillna(0)\n",
    "print(s.isna().any(axis=1).sum())\n",
    "print(s.isna().sum().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c6b271-c824-4f16-88cc-4a89c9fd4459",
   "metadata": {},
   "source": [
    "### 2.2 Removing Redundant Rows and Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ec4995-7a04-4674-83c3-d98b40215880",
   "metadata": {},
   "source": [
    "#### Dropping Rows and Features with Too less value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9b0643-1643-48b6-b765-341a13352c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference for rows missing value threshold\n",
    "s_b.loc[49:55,:].isna().mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4a968e-13e6-41cb-9be7-eb74551caef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = s[s.isna().mean(axis=1) <= 0.55]\n",
    "print(s.isna().any(axis=1).sum())\n",
    "print(s.isna().sum().sum())\n",
    "print(s.isna().sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630db254-4478-4115-9e87-affacebc9306",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = s.loc[:, s.isna().mean() <= 0.40]\n",
    "print(s.isna().any(axis=1).sum())\n",
    "print(s.isna().sum().sum())\n",
    "print(s.isin(special_values).any(axis=1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b597ac7-e67a-4490-895c-2189f1d647ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "s.to_csv(\"Deliverable2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a4f1b7-7cf8-4e32-a5a3-d90e2c7ba2c2",
   "metadata": {},
   "source": [
    "### 2.3 Imputate Continuous Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbc4c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NCES placeholder codes (-1, -2, -9) are non observations (missing / not applicable / low quality),\n",
    "# so they shouldn't be used as real numeric values in statistics and be replaced.\n",
    "\n",
    "placeholder_codes = [-1, -2, -9, \"M\", \"N\"]\n",
    "\n",
    "# Replace placeholder codes with NaN (only for columns where codes appear)\n",
    "for c in [\"STUTERATIO\", \"TOTFRL\", \"FRELCH\", \"REDLCH\", \"DIRECTCERT\"]:\n",
    "    if c in s.columns:\n",
    "        s[c] = s[c].replace(placeholder_codes, np.nan)\n",
    "\n",
    "# Ensure numeric columns are numeric\n",
    "numeric_fix_cols = [\n",
    "    \"TOTAL\", \"FTE\", \"STUTERATIO\",\n",
    "    \"TOTFRL\", \"FRELCH\", \"REDLCH\", \"DIRECTCERT\"\n",
    "]\n",
    "for c in numeric_fix_cols:\n",
    "    if c in s.columns:\n",
    "        s[c] = pd.to_numeric(s[c], errors=\"coerce\")\n",
    "\n",
    "print(\"Missing counts (after placeholder -> NaN):\")\n",
    "print(s[numeric_fix_cols].isna().sum().sort_values(ascending=False))\n",
    "\n",
    "# TOTAL: If TOTAL is missing, rebuild from grade level columns (PK to AE) when possible.\n",
    "grade_cols = [c for c in s.columns if c in list(s.loc[:, \"PK\":\"AE\"].columns)]\n",
    "if \"TOTAL\" in s.columns and grade_cols:\n",
    "    total_from_grades = s[grade_cols].sum(axis=1, min_count=1)\n",
    "    missing_total = s[\"TOTAL\"].isna()\n",
    "    s.loc[missing_total, \"TOTAL\"] = total_from_grades[missing_total]\n",
    "    print(\"Filled TOTAL from grade sums:\", int(missing_total.sum()))\n",
    "\n",
    "# STUTERATIO: If missing, compute using TOTAL and FTE when both exist.\n",
    "# First, we check that the relationship holds for most records.\n",
    "if \"TOTAL\" in s.columns and \"FTE\" in s.columns and \"STUTERATIO\" in s.columns:\n",
    "    s[\"RATIO_FROM_TOTAL_FTE\"] = s[\"TOTAL\"] / s[\"FTE\"]\n",
    "    s.loc[s[\"FTE\"] == 0, \"RATIO_FROM_TOTAL_FTE\"] = None\n",
    "    temp = s.dropna(subset=[\"STUTERATIO\", \"RATIO_FROM_TOTAL_FTE\"])\n",
    "    temp[\"ABS_DIFF\"] = (temp[\"STUTERATIO\"] - temp[\"RATIO_FROM_TOTAL_FTE\"]).abs()\n",
    "    print(\"mean abs diff =\", round(float(temp[\"ABS_DIFF\"].mean()), 4))\n",
    "    before = s[\"STUTERATIO\"].isna().sum()\n",
    "    s.loc[s[\"STUTERATIO\"].isna(), \"STUTERATIO\"] = s[\"RATIO_FROM_TOTAL_FTE\"]\n",
    "    after = s[\"STUTERATIO\"].isna().sum()\n",
    "    print(\"Filled STUTERATIO values:\", int(before - after))\n",
    "    s = s.drop(columns=[\"RATIO_FROM_TOTAL_FTE\"], errors=\"ignore\")\n",
    "# To verify consistency internally, we compared the STUTERATIO to the computed value TOTAL/FTE. \n",
    "# We calculated the mean absolute difference to measure the avg deviation between the two values. \n",
    "# A small mean absolute difference shows that the ratios are consistent with actual enrollment and teacher counts.\n",
    "\n",
    "# FTE: If missing, compute using TOTAL / STUTERATIO.\n",
    "if \"TOTAL\" in s.columns and \"FTE\" in s.columns and \"STUTERATIO\" in s.columns:\n",
    "    s[\"FTE_FROM_RATIO\"] = s[\"TOTAL\"] / s[\"STUTERATIO\"]\n",
    "    s.loc[s[\"STUTERATIO\"] == 0, \"FTE_FROM_RATIO\"] = None\n",
    "    before = s[\"FTE\"].isna().sum()\n",
    "    s.loc[s[\"FTE\"].isna(), \"FTE\"] = s[\"FTE_FROM_RATIO\"]\n",
    "    after = s[\"FTE\"].isna().sum()\n",
    "    print(\"Filled FTE values:\", int(before - after))\n",
    "    s = s.drop(columns=[\"FTE_FROM_RATIO\"], errors=\"ignore\")\n",
    "\n",
    "# Remaining missing in continuous columns: use group wise median since less sensitive to outliers\n",
    "# Group by SCHOOL_LEVEL and STABR if available because school staffing and enrollment patterns are different depending on level and state.\n",
    "for c in [\"STUTERATIO\", \"FTE\"]:\n",
    "    if c in s.columns:\n",
    "        # If both grouping columns exist, fill by SCHOOL_LEVEL and STABR\n",
    "        if \"SCHOOL_LEVEL\" in s.columns and \"STABR\" in s.columns:\n",
    "            for level in s[\"SCHOOL_LEVEL\"].dropna().unique():\n",
    "                for state in s[\"STABR\"].dropna().unique():\n",
    "                    # rows in this level and this state\n",
    "                    condition = (s[\"SCHOOL_LEVEL\"] == level) & (s[\"STABR\"] == state)\n",
    "                    group_values = s.loc[condition, c]\n",
    "                    # calculate median if the group has at least one proper value\n",
    "                    if group_values.notna().sum() > 0:\n",
    "                        group_median = group_values.median()\n",
    "                        # fill only missing values inside this group\n",
    "                        s.loc[condition & s[c].isna(), c] = group_median\n",
    "        # Otherwise, fill it with overall median\n",
    "        else:\n",
    "            overall_median = s[c].median()\n",
    "            s.loc[s[c].isna(), c] = overall_median\n",
    "# Remaining missing values in continuous variables (STUTERATIO and FTE) were imputed using group-wise median imputation. \n",
    "# Schools were grouped by SCHOOL_LEVEL and STABR because staffing and enrollment differ across different school types and states. \n",
    "# Median was chosen instead of mean to minimize sensitivity to extreme outliers in the enrollment and staffing counters. \n",
    "# This helps to uphold differences in the structure of the dataset but preventing too much data loss from deleting rows. \n",
    "# If grouping variables were unavailable, overall median imputation was used.\n",
    "\n",
    "print(\"\\nMissing counts after continuous imputations:\")\n",
    "print(s[[\"TOTAL\", \"FTE\", \"STUTERATIO\"]].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e163744b-1eaa-445c-8e83-844c0ba72a4f",
   "metadata": {},
   "source": [
    "### 2.4 Imputate Discrete Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e01aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Fix postal codes: if LZIP is numeric, leading zeros are lost in python. This converts them back to 5 character string.\n",
    "if \"LZIP\" in s.columns:\n",
    "    s[\"LZIP\"] = s[\"LZIP\"].astype(\"Int64\").astype(str).str.zfill(5)\n",
    "\n",
    "# Address field: if missing, fill with 'Unknown'\n",
    "for c in [\"LSTREET1\", \"PHONE\"]:\n",
    "    if c in s.columns:\n",
    "        s[c] = s[c].fillna(\"Unknown\")\n",
    "# Missing values in categorical variables not included in the analysis such as street address and phone number were replaced \n",
    "# with the placeholder value \"Unknown\". These variables are descriptive identifiers and are not used for analysis. \n",
    "# Replacing missing values prevents needing to delete rows unnecessarily due to empty values and ensures consistency in the dataset.\n",
    "\n",
    "# Free/Reduced lunch fields: these are count fields and can be 0 for schools with no participants.\n",
    "# We used enrollment percentage as a consideration to avoid producing impossible counts.\n",
    "if \"TOTFRL\" in s.columns and \"TOTAL\" in s.columns:\n",
    "    # Create FRL_PCT (only when TOTAL > 0)\n",
    "    s[\"FRL_PCT\"] = s[\"TOTFRL\"] / s[\"TOTAL\"]\n",
    "    s.loc[s[\"TOTAL\"] == 0, \"FRL_PCT\"] = None\n",
    "    # Fill missing FRL_PCT with overall median\n",
    "    frl_median = s[\"FRL_PCT\"].median()\n",
    "    s.loc[s[\"FRL_PCT\"].isna(), \"FRL_PCT\"] = frl_median\n",
    "    # missing TOTFRL using FRL_PCT * TOTAL\n",
    "    s.loc[s[\"TOTFRL\"].isna(), \"TOTFRL\"] = (s[\"FRL_PCT\"] * s[\"TOTAL\"]).round()\n",
    "    # keep TOTFRL within valid range [0, TOTAL]\n",
    "    s.loc[s[\"TOTFRL\"] < 0, \"TOTFRL\"] = 0\n",
    "    s.loc[s[\"TOTFRL\"] > s[\"TOTAL\"], \"TOTFRL\"] = s[\"TOTAL\"]\n",
    "# if FRELCH and REDLCH exist, fill missing by splitting TOTFRL 50/50\n",
    "if \"FRELCH\" in s.columns and \"REDLCH\" in s.columns and \"TOTFRL\" in s.columns:\n",
    "    # If missing, assign half of TOTFRL to free lunch\n",
    "    s.loc[s[\"FRELCH\"].isna(), \"FRELCH\"] = (0.5 * s[\"TOTFRL\"]).round()\n",
    "    # Reduced lunch is whatever is left\n",
    "    s.loc[s[\"REDLCH\"].isna(), \"REDLCH\"] = (s[\"TOTFRL\"] - s[\"FRELCH\"]).round()\n",
    "    # Prevent negatives\n",
    "    s.loc[s[\"REDLCH\"] < 0, \"REDLCH\"] = 0\n",
    "# The Free/Reduced Lunch variables are count fields, so they must be not negative and not greater than total enrollment.\n",
    "# To handle missing values first, we converted the total FRL count into a percentage of enrollment so that any imputed values \n",
    "# would scale properly with school size. Missing percentages were filled using the median to avoid being influenced by extreme values. \n",
    "# Then we rebuilt missing FRL counts using the percentage and total enrollment, rounding to keep whole numbers. \n",
    "# More checks made sure values stayed between 0 and total enrollment. When the free and reduced breakdown was missing,\n",
    "# we used a 50/50 split to maintain consistency. These steps ensured logical modification while keeping as much data as possible.\n",
    "\n",
    "# Ensure counts are non-negative\n",
    "count_cols = [\"TOTAL\", \"TOTFRL\", \"FRELCH\", \"REDLCH\", \"DIRECTCERT\", \"TOTMENROL\", \"TOTFENROL\", \"MEMBER\"]\n",
    "\n",
    "for c in count_cols:\n",
    "    if c in s.columns:\n",
    "        s[c] = pd.to_numeric(s[c], errors=\"coerce\")\n",
    "        s.loc[s[c] < 0, c] = 0\n",
    "\n",
    "print(\"Done discrete repairs. Remaining missing values (top 15):\")\n",
    "print(s.isna().sum().sort_values(ascending=False).head(15))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0466692-4de6-4097-9e71-1df3cfabff92",
   "metadata": {},
   "source": [
    "### 2.5 Handling Edge Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab8960d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: ensure totals are internally consistent and remove/repair impossible records. Sorry Monte partially gave up on this section LOL\n",
    "\n",
    "race_cols = [\"AM\",\"AS\",\"BL\",\"HI\",\"HP\",\"TR\",\"WH\"]\n",
    "race_cols = [c for c in race_cols if c in s.columns]\n",
    "\n",
    "# Convert to numeric\n",
    "for c in race_cols:\n",
    "    s[c] = pd.to_numeric(s[c], errors=\"coerce\")\n",
    "\n",
    "# Race counts: if ALL race columns are missing, ethnicity composition cannot be computed.\n",
    "# Since our problem statement uses ethnicity composition, we drop these rows.\n",
    "all_race_missing = s[race_cols].isna().all(axis=1)\n",
    "print(\"Rows with all race counts missing:\", int(all_race_missing.sum()))\n",
    "s = s.loc[~all_race_missing].copy()\n",
    "\n",
    "# If some race categories are missing but the known categories already sum to TOTAL, then missing categories must be 0.\n",
    "if \"TOTAL\" in s.columns:\n",
    "\n",
    "    race_sum_known = s[race_cols].sum(axis=1, min_count=1)\n",
    "\n",
    "    # Loop through each row\n",
    "    for i in s.index:\n",
    "        if (s.loc[i, race_cols].isna().any() and pd.notna(s.loc[i, \"TOTAL\"]) and race_sum_known.loc[i] == s.loc[i, \"TOTAL\"]):\n",
    "            s.loc[i, race_cols] = s.loc[i, race_cols].fillna(0)\n",
    "\n",
    "    # If exactly ONE race category is missing and TOTAL is known, fill the missing one as the remainder.\n",
    "    missing_counts = s[race_cols].isna().sum(axis=1)\n",
    "    one_missing = (missing_counts == 1) & s[\"TOTAL\"].notna()\n",
    "    remainder = s[\"TOTAL\"] - s[race_cols].sum(axis=1, min_count=1)\n",
    "\n",
    "    # Identify which column is missing per row and fill it\n",
    "    for c in race_cols:\n",
    "        mask = one_missing & s[c].isna() & (remainder >= 0)\n",
    "        s.loc[mask, c] = remainder[mask]\n",
    "\n",
    "    # Any remaining negative remainder means race totals exceed TOTAL (inconsistent).\n",
    "    remainder_after = s[\"TOTAL\"] - s[race_cols].sum(axis=1, min_count=1)\n",
    "    inconsistent_race = remainder_after < 0\n",
    "    print(\"Rows where race sum exceeds TOTAL (dropped):\", int(inconsistent_race.sum()))\n",
    "    s = s.loc[~inconsistent_race].copy()\n",
    "\n",
    "# Gender totals vs TOTAL: if TOTMENROL + TOTFENROL != TOTAL, we keep as is but flag for awareness.\n",
    "if \"TOTMENROL\" in s.columns and \"TOTFENROL\" in s.columns and \"TOTAL\" in s.columns:\n",
    "    condition = (s[\"TOTMENROL\"].notna() & s[\"TOTFENROL\"].notna() & s[\"TOTAL\"].notna())\n",
    "    # Among those rows, check if male + female does NOT equal TOTAL\n",
    "    inconsistent = s.loc[condition, \"TOTMENROL\"] + s.loc[condition, \"TOTFENROL\"] != s.loc[condition, \"TOTAL\"]\n",
    "    print(\"Rows where male+female != TOTAL:\", int(inconsistent.sum()))\n",
    "    \n",
    "# Final missing snapshot\n",
    "print(\"\\nRemaining missing values (top 15):\")\n",
    "print(s.isna().sum().sort_values(ascending=False).head(15))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cea250-0a98-475e-a627-33d2d6ddf55b",
   "metadata": {},
   "source": [
    "### 2.6 Feature Engineering\n",
    "The features added through the imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6779b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ethnicity composition as proportions (robust to different school sizes)\n",
    "race_cols = [\"AM\",\"AS\",\"BL\",\"HI\",\"HP\",\"TR\",\"WH\"]\n",
    "race_cols = [c for c in race_cols if c in s.columns]\n",
    "\n",
    "for c in race_cols:\n",
    "    s[c] = pd.to_numeric(s[c], errors=\"coerce\")\n",
    "\n",
    "if \"TOTAL\" in s.columns:\n",
    "    for c in race_cols:\n",
    "        s[f\"{c}_PCT\"] = np.where(s[\"TOTAL\"] > 0, s[c] / s[\"TOTAL\"], np.nan)\n",
    "\n",
    "    # minority share (everything except White)\n",
    "    if \"WH\" in s.columns:\n",
    "        s[\"MINORITY_PCT\"] = 1 - s[\"WH_PCT\"]\n",
    "\n",
    "# School level (ordinal encoding) to support correlation\n",
    "level_map = {\n",
    "    \"Primary\": 1,\n",
    "    \"Middle\": 2,\n",
    "    \"High\": 3,\n",
    "    \"Other\": 0\n",
    "}\n",
    "if \"SCHOOL_LEVEL\" in s.columns:\n",
    "    s[\"SCHOOL_LEVEL_CODE\"] = s[\"SCHOOL_LEVEL\"].map(level_map).fillna(0).astype(int)\n",
    "\n",
    "print(\"Example engineered columns:\")\n",
    "engineered_cols = [c for c in s.columns if c.endswith(\"_PCT\")] + [\"MINORITY_PCT\",\"FRL_PCT\",\"IS_CHARTER\",\"IS_VIRTUAL\",\"SCHOOL_LEVEL_CODE\"]\n",
    "print([c for c in engineered_cols if c in s.columns][:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8044204f-9b9e-4fc9-b440-f5f241045095",
   "metadata": {},
   "source": [
    "### 2.7 Dropping Constant Features and Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f03383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop fully-constant columns providing no information for model\n",
    "nunique = s.nunique(dropna=False)\n",
    "constant_cols = list(nunique[nunique <= 1].index)\n",
    "print(\"Constant columns:\", constant_cols)\n",
    "\n",
    "# >= 99.5% of rows share the same value\n",
    "quasi_constant_cols = []\n",
    "for col in s.columns:\n",
    "    top_freq = s[col].value_counts(dropna=False, normalize=True).iloc[0]\n",
    "    if top_freq >= 0.995:\n",
    "        quasi_constant_cols.append(col)\n",
    "\n",
    "print(\">=99.5% same:\", quasi_constant_cols)\n",
    "\n",
    "# Remove duplicate records (exact duplicates or duplicate school IDs)\n",
    "before = s.shape[0]\n",
    "s = s.drop_duplicates()\n",
    "after = s.shape[0]\n",
    "print(f\"Exact duplicate rows removed: {before-after}\")\n",
    "\n",
    "if \"NCESSCH\" in s.columns:\n",
    "    before = s.shape[0]\n",
    "    s = s.drop_duplicates(subset=[\"NCESSCH\"])\n",
    "    after = s.shape[0]\n",
    "    print(f\"Duplicate NCESSCH rows removed: {before-after}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8986a41d-dcc4-47b7-a92b-cc912f84b787",
   "metadata": {},
   "source": [
    "### 2.8 Data Shape After Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744aa415",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape after cleaning:\", s.shape)\n",
    "print(\"Total missing values remaining:\", int(s.isna().sum().sum()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb0d7b1-d112-4e49-ae54-317cf52848c1",
   "metadata": {},
   "source": [
    "## Step 3: Normalization and Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f294a441",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Transformation (log1p) for heavy-tailed count features to reduce skew\n",
    "\n",
    "# Standardization (z-score) for continuous features used in modeling\n",
    "\n",
    "# Select a few representative count features\n",
    "count_features = [c for c in [\"TOTAL\",\"FTE\",\"TOTFRL\"] if c in s.columns]\n",
    "for c in count_features:\n",
    "    s[f\"LOG1P_{c}\"] = np.log1p(s[c])\n",
    "\n",
    "# Standardize continuous features\n",
    "cont_features = [c for c in [\"STUTERATIO\",\"FRL_PCT\",\"MINORITY_PCT\",\"TOTAL\",\"FTE\"] if c in s.columns]\n",
    "for c in cont_features:\n",
    "    mean = s[c].mean()\n",
    "    std = s[c].std()\n",
    "    s[f\"Z_{c}\"] = (s[c] - mean) / std\n",
    "\n",
    "# Correlation analysis: which engineered variables correlate with STUTERATIO?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c2c7ae",
   "metadata": {},
   "source": [
    "### 3.1 Export Cleaned Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d11da9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "s.to_csv(\"Deliverable2_cleaned.csv\", index=False)\n",
    "print(\"Saved Deliverable2_cleaned_final.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
